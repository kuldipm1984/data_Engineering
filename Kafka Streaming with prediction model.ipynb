{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8934478b-6f22-4f7c-abdb-35af59d7d5d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mystore.dfs.core.windows.net\",\n",
    "    \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cd422c-da80-48e7-bcb3-4fbaea70f19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Event Hub config \n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "eh_namespace = \"databevent\"\n",
    "eh_name = \"datab\"\n",
    "\n",
    "eh_connection_string = (\n",
    "    \"Endpoint=sb://databevent.servicebus.windows.net/;\"\n",
    "    \"SharedAccessKeyName=RootManageSharedAccessKey;\"\n",
    "    \"SharedAccessKey=XXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "eh_sasl = (\n",
    "    'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required '\n",
    "    'username=\"$ConnectionString\" '\n",
    "    f'password=\"{eh_connection_string}\";'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8f0804-8db6-40ad-ab02-a4b2893d5873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema \n",
    "schema = StructType() \\\n",
    "    .add(\"Temperature\", IntegerType()) \\\n",
    "    .add(\"Humidity\", IntegerType()) \\\n",
    "    .add(\"WindSpeed\", DoubleType()) \\\n",
    "    .add(\"Precipitation\", IntegerType()) \\\n",
    "    .add(\"AtmosphericPressure\", DoubleType()) \\\n",
    "    .add(\"UVIndex\", IntegerType()) \\\n",
    "    .add(\"Season\", StringType()) \\\n",
    "    .add(\"Visibility\", DoubleType()) \\\n",
    "    .add(\"Location\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9801a9-f7e1-44dd-b398-035f262e00b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{eh_namespace}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", eh_name)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.sasl.jaas.config\", eh_sasl)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0ab24e-5eaf-4d28-bef8-0b3df8f1608d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse JSON\n",
    "df_parsed = (\n",
    "    df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "      .select(from_json(col(\"json_str\"), schema).alias(\"data\"))\n",
    "      .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6958f2df-e105-4f16-9cf5-13f5c7bebea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7ff87042cd70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to MANAGED DELTA TABLE\n",
    "(\n",
    "    df_parsed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/delta/checkpoints/weather_stream\")\n",
    "    .table(\"default.weather_stream_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd90bf93-edd7-4e60-a36e-bb063cfb4877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>AtmosphericPressure</th>\n",
       "      <th>UVIndex</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>990.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2.5</td>\n",
       "      <td>mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>990.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>10.0</td>\n",
       "      <td>coastal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1010.82</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1010.03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity  WindSpeed  ...  Season  Visibility  Location\n",
       "0         27.0      74.0       17.0  ...  Autumn         2.5  mountain\n",
       "1         29.0      45.0        8.5  ...  Spring        10.0   coastal\n",
       "2         20.0      73.0        9.5  ...  Winter         3.5    inland\n",
       "3         32.0      55.0        3.5  ...  Summer         5.0    inland\n",
       "4          NaN       NaN        NaN  ...    None         NaN      None\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+display_df_hint": {
       "mode": "should_hint",
       "name": "x_new_df",
       "type": "pandas.core.frame.DataFrame"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Table as dataframe\n",
    "x_new_df= spark.table('weather_stream_table').toPandas()\n",
    "x_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a7077a-038d-431f-bb57-8d1e9e5801c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 17:10:39.978472: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2026-01-07 17:10:40.008199: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-01-07 17:10:40.119783: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-01-07 17:10:40.244770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767805840.360116    1382 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767805840.384269    1382 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767805840.631259    1382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767805840.631290    1382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767805840.631302    1382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767805840.631304    1382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2026-01-07 17:10:40.664753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/databricks/python/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n2026-01-07 17:10:52.652823: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\nWARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n/databricks/python/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/databricks/python/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load locally developed Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "dbutils.fs.cp(\n",
    "  \"abfss://rawdata@mystore.dfs.core.windows.net/model/\",\n",
    "  \"dbfs:/FileStore/modelh5/\",\n",
    "  recurse=True\n",
    ")\n",
    "model = tf.keras.models.load_model(\"/dbfs/FileStore/modelh5/weather_classifier.h5\")\n",
    "sc = pickle.load(open(\"/dbfs/FileStore/modelh5/sc.pkl\", \"rb\"))\n",
    "le_season = pickle.load(open(\"/dbfs/FileStore/modelh5/le_season.pkl\", \"rb\"))\n",
    "le_location = pickle.load(open(\"/dbfs/FileStore/modelh5/le_location.pkl\", \"rb\"))\n",
    "le_wt = pickle.load(open(\"/dbfs/FileStore/modelh5/le_wt.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac49fe50-4e66-4f4d-8b76-87fa29cf2d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 177ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 178ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "x_new_df= x_new_df.loc[0:3]\n",
    "x_new_numerical = x_new_df[['Temperature','Humidity','WindSpeed','Precipitation','AtmosphericPressure','UVIndex','Visibility']]\n",
    "x_new_numerical_scaled = sc.transform(x_new_numerical)\n",
    "x_new_df['Season'] = le_season.transform(x_new_df['Season'])   \n",
    "x_new_df['Location'] = le_location.transform(x_new_df['Location'])\n",
    "x_new_season = x_new_df['Season'].values\n",
    "x_new_location = x_new_df['Location'].values\n",
    "val_preds = model.predict([x_new_numerical_scaled,x_new_season,x_new_location])\n",
    "predicted_indices = np.argmax(val_preds, axis=1)\n",
    "predicted_labels = le_wt.inverse_transform(predicted_indices)\n",
    "x_new_df = x_new_df.copy()  \n",
    "x_new_df[\"predicted_value\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1477a32-c440-481a-88c9-531b92524f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x_new_df['Season'] = le_season.inverse_transform(x_new_df['Season'])   \n",
    "x_new_df['Location'] = le_location.inverse_transform(x_new_df['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d278b3a-6f11-4717-9376-0e3c7bb55d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>AtmosphericPressure</th>\n",
       "      <th>UVIndex</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Location</th>\n",
       "      <th>predicted_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>990.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2.5</td>\n",
       "      <td>mountain</td>\n",
       "      <td>Windy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>990.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>10.0</td>\n",
       "      <td>coastal</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1010.82</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.5</td>\n",
       "      <td>inland</td>\n",
       "      <td>Windy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1010.03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inland</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity  WindSpeed  ...  Visibility  Location  predicted_value\n",
       "0         27.0      74.0       17.0  ...         2.5  mountain            Windy\n",
       "1         29.0      45.0        8.5  ...        10.0   coastal           Cloudy\n",
       "2         20.0      73.0        9.5  ...         3.5    inland            Windy\n",
       "3         32.0      55.0        3.5  ...         5.0    inland           Cloudy\n",
       "\n",
       "[4 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+display_df_hint": {
       "mode": "should_hint",
       "name": "x_new_df",
       "type": "pandas.core.frame.DataFrame"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_new_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4888740847092695,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "kafkastream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}